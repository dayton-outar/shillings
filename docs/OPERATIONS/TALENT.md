# Talent Management

Talent management is the strategic process organizations use to attract, develop, motivate, and retain skilled employees to achieve business goals. It involves identifying key roles, sourcing and recruiting top talent, providing training and development, and fostering career growth. Effective talent management aligns individual employee growth with the organization's needs, creating a supportive work environment that maximizes both employee satisfaction and company performance. By focusing on long-term workforce planning, companies can build a competitive edge and ensure sustainable success.

## Management Values and Culture

Managing talent involves communicating values in order to foster a culture.

### Collaborative Nirvana

> To reach collaborative nirvana, you first need to learn and embrace what I call the “three pillars” of social skills. These three principles aren’t just about greasing the wheels of relationships; they’re the foundation on which all healthy interaction and collaboration are based:
> 
> ***Pillar 1: Humility***
>
> You are not the center of the universe (nor is your code!). You’re neither omniscient nor infallible. You’re open to self-improvement.
> 
> ***Pillar 2: Respect***
> 
> You genuinely care about others you work with. You treat them kindly and appreciate their abilities and accomplishments.
> 
> ***Pillar 3: Trust***
> 
> You genuinely care about others you work with. You treat them kindly and appreciate their abilities and accomplishments.
> 
> ...
> 
> In a professional software engineering environment, criticism is almost never personal—it’s usually just part of the process of making a better project. The trick is to make sure you (and those around you) understand the difference between a constructive criticism of someone’s creative output and a flat-out assault against someone’s character. The latter is useless—it’s petty and nearly impossible to act on. The former can (and should!) be helpful and give guidance on how to improve. And, most important, it’s imbued with respect: the person giving the constructive criticism genuinely cares about the other person and wants them to improve themselves or their work. Learn to respect your peers and give constructive criticism politely. If you truly respect someone, you’ll be motivated to choose tactful, helpful phrasing—a skill acquired with much practice.
> 
> ...
> 
> Over in Google X—the division that works on “moonshots” like self-driving cars and internet access delivered by balloons—failure is deliberately built into its incentive system. People come up with outlandish ideas and coworkers are actively encouraged to shoot them down as fast as possible. Individuals are rewarded (and even compete) to see how many ideas they can disprove or invalidate in a fixed period of time. Only when a concept truly cannot be debunked at a whiteboard by all peers does it proceed to early prototype.
> 
> ...
> 
> ... “Googleyness”—a set of attributes and behaviors that we look for that represent strong leadership and exemplify “humility, respect, and trust”:
> 
> _Thrives in ambiguity_
> 
> Can deal with conflicting messages or directions, build consensus, and make progress against a problem, even when the environment is constantly shifting.
> 
> _Values feedback_
> 
> Has humility to both receive and give feedback gracefully and understands how valuable feedback is for personal (and team) development.
> 
> _Challenges status quo_ 
> 
> Is able to set ambitious goals and pursue them even when there might be resistance or inertia from others.
> 
> _Puts the user first_ 
> 
> Has empathy and respect for users of Google’s products and pursues actions that are in their best interests.
> 
> _Cares about the team_ 
> 
> Has empathy and respect for coworkers and actively works to help them without being asked, improving team cohesion.
> 
> _Does the right thing_ 
> 
> Has a strong sense of ethics about everything they do; willing to make difficult or inconvenient decisions to protect the integrity of the team and product.
> [^1]

### Blameless Post-Mortem Culture

> The key to learning from your mistakes is to document your failures by performing a root-cause analysis and writing up a “postmortem,” as it’s called at Google (and many other companies). Take extra care to make sure the postmortem document isn’t just a useless list of apologies or excuses or finger-pointing—that’s not its purpose. A proper postmortem should always contain an explanation of what was learned and what is going to change as a result of the learning experience. Then, make sure that the post‐mortem is readily accessible and that the team really follows through on the proposed changes. Properly documenting failures also makes it easier for other people (present and future) to know what happened and avoid repeating history. Don’t erase your tracks—light them up like a runway for those who follow you!
> 
> A good postmortem should include the following:
> - A brief summary of the event
> - A timeline of the event, from discovery through investigation to resolution
> - The primary cause of the event
> - Impact and damage assessment
> - A set of action items (with owners) to fix the problem immediately
> - A set of action items to prevent the event from happening again
> - Lessons learned [^1]

### Netflix's Culture

Patty McCord’s book *"Powerful: Building a Culture of Freedom and Responsibility"* outlines key insights drawn from her experience as Chief Talent Officer at Netflix, where she helped shape the company’s highly successful corporate culture. The most significant points made in the book include:

#### 1. **People Over Process**
McCord argues that many companies place too much emphasis on formal processes, rules, and policies at the expense of trusting their employees. She advocates for hiring smart, creative people and giving them the freedom to do their jobs without micromanagement.

#### 2. **Radical Honesty**
McCord promotes a culture of radical honesty, where employees and managers give direct, candid feedback. She believes regular, honest communication leads to better outcomes and that sugarcoating feedback does a disservice to employees.

#### 3. **The End of the Traditional Performance Review**
McCord criticizes the traditional annual performance review process, calling it ineffective. She encourages regular, informal conversations about performance, which can help employees grow and improve continuously rather than waiting for an annual evaluation.

#### 4. **Embrace Change**
The book emphasizes that companies and employees must be adaptable in a fast-paced world. McCord suggests that companies should be willing to make bold changes, whether it involves restructuring teams, changing strategies, or letting go of employees whose skills no longer match company needs.

#### 5. **Company Culture is a Living Process**
McCord rejects the idea of rigid corporate values and instead believes that culture should evolve as the company grows. She views culture as a living process, reflecting the company’s current objectives and the people working toward them.

#### 6. **Focus on High Performance, Not Retention**
Rather than focusing on employee retention for its own sake, McCord advocates building teams of high-performing individuals who are well-suited for the company’s current needs. Sometimes this means letting go of employees whose skills no longer align with company goals, but doing so humanely and with transparency.

#### 7. **Let Employees Own Their Career Growth**
McCord advises against coddling employees or managing their career development too closely. Instead, she suggests companies give employees the tools and feedback they need to grow while letting them take responsibility for their own career progression.

#### 8. **Hire Adults, Not Kids**
She challenges the conventional view of managing employees, treating them as adults who can handle the truth about performance and decisions. She encourages trust and transparency from leadership, believing that employees should be treated with respect, as mature professionals.

In *"Powerful"*, McCord outlines a vision of a modern corporate culture that prioritizes trust, accountability, and high performance over rigid processes and policies.

### Google's Culture

Laszlo Bock’s book *"Work Rules!: Insights from Inside Google That Will Transform How You Live and Lead"* provides a deep dive into the management and HR practices that shaped Google’s success. As the former Senior Vice President of People Operations at Google, Bock shares key insights about creating an empowering and innovative workplace. Here are the most significant points from the book:

#### 1. **Trust in People**
Bock emphasizes that trusting employees is key to building a successful organization. He argues that when companies trust their people to make decisions and take responsibility, they will rise to the occasion. This is exemplified by Google's policy of offering employees significant freedom, such as the famous "20% time" initiative where employees can work on passion projects.

#### 2. **Data-Driven HR Decisions**
At Google, data is used extensively to inform decisions, including those related to hiring, compensation, and promotions. Bock advocates for making HR decisions based on data and analytics rather than relying on intuition or tradition. He shares examples of how Google collects and uses data to identify what works best in managing and motivating employees.

#### 3. **Hiring is Everything**
Bock believes hiring the right people is one of the most important tasks in any company. Google is known for its rigorous hiring process, which focuses on finding smart, adaptable people with a high capacity for learning rather than specific skills or experience. Bock stresses that companies should focus on hiring people who will thrive in their environment, even if it means taking longer to fill roles.

#### 4. **Nurture a Learning Culture**
Bock encourages companies to create a culture of continuous learning and development. Google invests heavily in helping its employees grow through training, mentoring, and opportunities for personal development. Bock argues that when employees are given room to grow, they become more engaged, productive, and loyal.

#### 5. **Give Employees a Voice**
One of Google’s core principles is giving employees a voice in decision-making. Bock believes that involving employees in decisions—through open forums, feedback channels, and transparency—leads to better outcomes. He also advocates for empowering employees to challenge the status quo and offer new ideas.

#### 6. **Be Transparent**
Bock emphasizes the importance of transparency within an organization. At Google, information is widely shared with employees, from financial results to strategic plans. This transparency fosters trust and empowers employees to make informed decisions aligned with the company’s goals.

#### 7. **Focus on Employee Well-Being**
Google is famous for offering perks like free food, wellness programs, and flexible workspaces. While these perks get attention, Bock explains that the real focus is on improving employee well-being. When employees feel valued and cared for, they are more engaged and productive. Bock encourages companies to consider the whole person—physical, mental, and emotional—when designing their workplace environment.

#### 8. **Don’t Be Afraid to Break Traditional Rules**
Bock argues that many traditional management practices are outdated and counterproductive. He encourages companies to break away from conventional norms, such as hierarchical decision-making and rigid performance evaluations. Instead, he advocates for a more flexible, innovative approach to managing people, where autonomy and experimentation are key.

#### 9. **Performance Management Should Focus on Development**
Bock critiques traditional performance management systems, arguing that they often fail to help employees improve. Instead, Google’s approach is to focus on regular feedback, coaching, and mentoring. Performance reviews should aim to help employees grow, rather than just evaluating past performance.

#### 10. **Pay Unfairly**
One of the more controversial points in the book is Bock’s argument that companies should not strive for "fair" pay. Instead, he believes in paying top performers significantly more than average performers because their contributions are exponentially greater. This approach, though unconventional, aligns with Google’s data-driven compensation practices, rewarding those who have the greatest impact.

#### 11. **Mission-Driven Culture**
Finally, Bock emphasizes the importance of having a strong mission and purpose. Google’s mission—"to organize the world’s information and make it universally accessible and useful"—drives everything the company does. Bock argues that when employees connect with a higher purpose, they are more motivated and engaged.

In *"Work Rules!"*, Laszlo Bock provides a detailed look at how to create an innovative, people-centered organization by challenging traditional management practices and fostering a culture of trust, transparency, and continuous improvement. His insights offer practical advice for building a workplace that attracts and retains top talent while driving long-term success.

## Continuous Feedback

Ideas on how to get feedback for key attitudes and meeting objectives.

### Why Should We Measure Engineering Productivity?

> Let’s presume that you have a thriving business (e.g., you run an online search engine), and you want to increase your business’s scope (enter into the enterprise application market, or the cloud market, or the mobile market). Presumably, to increase the scope of your business, you’ll need to also increase the size of your engineering organization. However, ***as organizations grow in size linearly, communication costs grow quadratically***. Adding more people will be necessary to increase the scope of your business, but the communication overhead costs will not scale linearly as you add additional personnel. As a result, you won’t be able to scale the scope of your business linearly to the size of your engineering organization.
> 
> There is another way to address our scaling problem, though: _we could make each individual more productive_. If we can increase the productivity of individual engineers in the organization, we can increase the scope of our business without the commensurate increase in communication overhead.
> 
> Google has had to grow quickly into new businesses, which has meant learning how to make our engineers more productive. To do this, we needed to understand what makes them productive, identify inefficiencies in our engineering processes, and fix the identified problems. Then, we would repeat the cycle as needed in a continuous improvement loop. By doing this, we would be able to scale our engineering organization with the increased demand on it.
> 
> However, this improvement cycle also takes human resources. It would not be worthwhile to improve the productivity of your engineering organization by the equivalent of 10 engineers per year if it took 50 engineers per year to understand and fix productivity blockers. _Therefore, our goal is to not only improve software engineering productivity, but to do so efficiently_.
> 
> At Google, we addressed these trade-offs by creating a team of researchers dedicated to understanding engineering productivity. Our research team includes people from the software engineering research field and generalist software engineers, but we also include social scientists from a variety of fields, including cognitive psychology and behavioral economics. The addition of people from the social sciences allows us to not only study the software artifacts that engineers produce, but to also understand the human side of software development, including personal motivations, incentive structures, and strategies for managing complex tasks. The goal of the team is to take a data-driven approach to measuring and improving engineering productivity.
> 
> ..., we walk through how our research team achieves this goal. This begins with the triage process: there are many parts of software development that we can measure, but what should we measure? After a project is selected, we walk through how the research team identifies meaningful metrics that will identify the problematic parts of the process. Finally, we look at how Google uses these metrics to track improvements to productivity.
> 
> ..., we follow one concrete example posed by the C++ and Java language teams at Google: readability. For most of Google’s existence, these teams have managed the readability process at Google. The readability process was put in place in the early days of Google, before automatic formatters and linters that block submission were commonplace. The process itself is expensive to run because it requires hundreds of engineers performing readability reviews for other engineers in order to grant readability to them. Some engineers viewed it as an archaic hazing process that no longer held utility, and it was a favorite topic to argue about around the lunch table. The concrete question from the language teams was this: is the time spent on the readability process worthwhile?[^2]

### Triage: Is It Even Worth Measuring?

> Before we decide how to measure the productivity of engineers, we need to know when a metric is even worth measuring. The measurement itself is expensive: it takes people to measure the process, analyze the results, and disseminate them to the rest of the company. Furthermore, the measurement process itself might be onerous and slow down the rest of the engineering organization. Even if it is not slow, tracking progress might change engineers’ behavior, possibly in ways that mask the underlying issues. We need to measure and estimate smartly; although we don’t want to guess, we shouldn’t waste time and resources measuring unnecessarily.
> 
> At Google, we’ve come up with a series of questions to help teams determine whether it’s even worth measuring productivity in the first place. We first ask people to describe what they want to measure in the form of a concrete question; we find that the more concrete people can make this question, the more likely they are to derive benefit from the process. When the readability team approached us, its question was simple: are the costs of an engineer going through the readability process worth the benefits they might be deriving for the company?
> 
> We then ask them to consider the following aspects of their question:
> 
> _What result are you expecting, and why?_
> 
> Even though we might like to pretend that we are neutral investigators, we are not. We do have preconceived notions about what ought to happen. By acknowledging this at the outset, we can try to address these biases and prevent post hoc explanations of the results.
> 
> When this question was posed to the readability team, it noted that it was not sure. People were certain the costs had been worth the benefits at one point in time, but with the advent of autoformatters and static analysis tools, no one was entirely certain. There was a growing belief that the process now served as a hazing ritual. Although it might still provide engineers with benefits (and they had survey data showing that people did claim these benefits), it was not clear whether it was worth the time commitment of the authors or the reviewers of the code.
> 
> _If the data supports your expected result, what action will be taken?_
> 
> We ask this because if no action will be taken, there is no point in measuring. Notice that an action might in fact be “maintain the status quo” if there is a planned change that will occur if we didn’t have this result.
> 
> When asked about this, the answer from the readability team was straightforward: if the benefit was enough to justify the costs of the process, they would link to the research and the data on the FAQ about readability and advertise it to set expectations.
> 
> _If we get a negative result, will appropriate action be taken?_
> 
> We ask this question because in many cases, we find that a negative result will not change a decision. There might be other inputs into a decision that would override any negative result. If that is the case, it might not be worth measuring in the first place. This is the question that stops most of the projects that our research team takes on; we learn that the decision makers were interested in knowing the results, but for other reasons, they will not choose to change course.
> 
> In the case of readability, however, we had a strong statement of action from the team. It committed that, if our analysis showed that the costs either outweighed the benefit or the benefits were negligible, the team would kill the process. As different programming languages have different levels of maturity in formatters and static analyses, this evaluation would happen on a per-language basis.
> 
> _Who is going to decide to take action on the result, and when would they do it?_
> 
> We ask this to ensure that the person requesting the measurement is the one who is empowered to take action (or is doing so directly on their behalf). Ultimately, the goal of measuring our software process is to help people make business decisions. It’s important to understand who that individual is, including what form of data convinces them. Although the best research includes a variety of approaches (everything from structured interviews to statistical analyses of logs), there might be limited time in which to provide decision makers with the data they need. In those cases, it might be best to cater to the decision maker. Do they tend to make decisions by empathizing through the stories that can be retrieved from interviews? Do they trust survey results or logs data? Do they feel comfortable with complex statistical analyses? If the decider doesn’t believe the form of the result in principle, there is again no point in measuring the process.
> 
> In the case of readability, we had a clear decision maker for each programming language. Two language teams, Java and C++, actively reached out to us for assistance, and the others were waiting to see what happened with those languages first. The decision makers trusted engineers’ self-reported experiences for understanding happiness and learning, but the decision makers wanted to see “hard numbers” based on logs data for velocity and code quality. This meant that we needed to include both qualitative and quantitative analysis for these metrics. There was not a hard deadline for this work, but there was an internal conference that would make for a useful time for an announcement if there was going to be a change. That deadline gave us several months in which to complete the work.
> 
> By asking these questions, we find that in many cases, measurement is simply not worthwhile … and that’s OK! There are many good reasons to not measure the impact of a tool or process on productivity. Here are some examples that we’ve seen:
> 
> _You can’t afford to change the process/tools right now_
> 
> There might be time constraints or financial constraints that prevent this. For example, you might determine that if only you switched to a faster build tool, it would save hours of time every week. However, the switchover will mean pausing development while everyone converts over, and there’s a major funding deadline approaching such that you cannot afford the interruption. Engineering trade-offs are not evaluated in a vacuum—in a case like this, it’s important to realize that the broader context completely justifies delaying action on a result.
> 
> _Any results will soon be invalidated by other factors_
> 
> Examples here might include measuring the software process of an organization just before a planned reorganization. Or measuring the amount of technical debt for a deprecated system.
> 
> The decision maker has strong opinions, and you are unlikely to be able to provide a large enough body of evidence, of the right type, to change their beliefs.
> 
> This comes down to knowing your audience. Even at Google, we sometimes find people who have unwavering beliefs on a topic due to their past experiences. We have found stakeholders who never trust survey data because they do not believe self-reports. We’ve also found stakeholders who are swayed best by a compelling narrative that was informed by a small number of interviews. And, of course, there are stakeholders who are swayed only by logs analysis. In all cases, we attempt to triangulate on the truth using mixed methods, but if a stakeholder is limited to believing only in methods that are not appropriate for the problem, there is no point in doing the work.
> 
> _The results will be used only as vanity metrics to support something you were going to do anyway_
> 
> This is perhaps the most common reason we tell people at Google not to measure a software process. Many times, people have planned a decision for multiple reasons, and improving the software development process is only one benefit of several. For example, the release tool team at Google once requested a measurement to a planned change to the release workflow system. Due to the nature of the change, it was obvious that the change would not be worse than the current state, but they didn’t know if it was a minor improvement or a large one. We asked the team: if it turns out to only be a minor improvement, would you spend the resources to implement the feature anyway, even if it didn’t look to be worth the investment? The answer was yes! The feature happened to improve productivity, but this was a side effect: it was also more performant and lowered the release tool team’s maintenance burden.
> 
> _The only metrics available are not precise enough to measure the problem and can be confounded by other factors_
> 
> In some cases, the metrics needed (see the upcoming section on how to identify metrics) are simply unavailable. In these cases, it can be tempting to measure using other metrics that are less precise (lines of code written, for example). However, any results from these metrics will be uninterpretable. If the metric confirms the stakeholders’ preexisting beliefs, they might end up proceeding with their plan without consideration that the metric is not an accurate measure. If it does not confirm their beliefs, the imprecision of the metric itself provides an easy explanation, and the stakeholder might, again, proceed with their plan.
> 
> When you are successful at measuring your software process, you aren’t setting out to prove a hypothesis correct or incorrect; _success means giving a stakeholder the data they need to make a decision_. If that stakeholder won’t use the data, the project is always a failure. We should only measure a software process when a concrete decision will be made based on the outcome. For the readability team, there was a clear decision to be made. If the metrics showed the process to be beneficial, they would publicize the result. If not, the process would be abolished. Most important, the readability team had the authority to make this decision.[^2]

### Selecting Meaningful Metrics with Goals and Signals

> At Google, we use the Goals/Signals/Metrics (GSM) framework to guide metrics creation.
> 
> - A _goal_ is a desired end result. It’s phrased in terms of what you want to understand at a high level and should not contain references to specific ways to measure it.
> 
> - A _signal_ is how you might know that you’ve achieved the end result. Signals are things we would like to measure, but they might not be measurable themselves.
> 
> - A _metric_ is proxy for a signal. It is the thing we actually can measure. It might not be the ideal measurement, but it is something that we believe is close enough.
> 
> The GSM framework encourages several desirable properties when creating metrics. First, by creating goals first, then signals, and finally metrics, it prevents the _streetlight effect_. The term comes from the full phrase “looking for your keys under the streetlight”: if you look only where you can see, you might not be looking in the right place. With metrics, this occurs when we use the metrics that we have easily accessible and that are easy to measure, regardless of whether those metrics suit our needs. Instead, GSM forces us to think about which metrics will actually help us achieve our goals, rather than simply what we have readily available.
> 
> Second, GSM helps prevent both metrics creep and metrics bias by encouraging us to come up with the appropriate set of metrics, using a principled approach, _in advance_ of actually measuring the result. Consider the case in which we select metrics without a principled approach and then the results do not meet our stakeholders’ expectations. At that point, we run the risk that stakeholders will propose that we use different metrics that they believe will produce the desired result. And because we didn’t select based on a principled approach at the start, there’s no reason to say that they’re wrong! Instead, GSM encourages us to select metrics based on their ability to measure the original goals. Stakeholders can easily see that these metrics map to their original goals and agree, in advance, that this is the best set of metrics for measuring the outcomes.
> 
> Finally, GSM can show us where we have measurement coverage and where we do not. When we run through the GSM process, we list all our goals and create signals for each one. As we will see in the examples, not all signals are going to be measurable—and that’s OK! With GSM, at least we have identified what is not measurable. By identifying these missing metrics, we can assess whether it is worth creating new metrics or even worth measuring at all.
> 
> The important thing is to maintain _traceability_. For each metric, we should be able to trace back to the signal that it is meant to be a proxy for and to the goal it is trying to measure. This ensures that we know which metrics we are measuring and why we are measuring them.[^2]

### Goals

> A goal should be written in terms of a desired property, without reference to any metric. By themselves, these goals are not measurable, but a good set of goals is something that everyone can agree on before proceeding onto signals and then metrics.[^2]

This philosophy goes against SMART: Specific, Measurable, Achievable, Relevant, and Time-Bound. It appears to be more like FAST: Frequently Discussed, Ambitious, Specific, Transparent.

> According to conventional wisdom, goals should be specific, measurable, achievable, realistic, and time-bound. But SMART goals undervalue ambition, focus narrowly on individual performance, and ignore the importance of discussing goals throughout the year. To drive strategy execution, leaders should instead set goals that are FAST — frequently discussed, ambitious, specific, and transparent.[^3]

![Leadership Communication Tips](/.attachments/leadership-communication-tips.jpeg)

> To make this work, we need to have identified the correct set of goals to measure in the first place. This would seem straightforward: surely the team knows the goals of their work! However, our research team has found that in many cases, people forget to include all the possible trade-offs within productivity, which could lead to mismeasurement.
>
> Taking the readability example, let’s assume that the team was so focused on making the readability process fast and easy that it had forgotten the goal about code quality. The team set up tracking measurements for how long it takes to get through the review process and how happy engineers are with the process. One of our teammate proposes the following:
> 
> _I can make your review velocity very fast: just remove code reviews entirely._
> 
> Although this is obviously an extreme example, teams forget core trade-offs all the time when measuring: they become so focused on improving velocity that they forget to measure quality (or vice versa). To combat this, our research team divides productivity into five core components. These five components are in trade-off with one another, and we encourage teams to consider goals in each of these components to ensure that they are not inadvertently improving one while driving others downward. To help people remember all five components, we use the mnemonic “QUANTS”:
> 
> _**Qu**ality of the code_
> 
> What is the quality of the code produced? Are the test cases good enough to prevent regressions? How good is an architecture at mitigating risk and changes?
> 
> _**A**ttention from engineers_
> 
> How frequently do engineers reach a state of flow? How much are they distracted by notifications? Does a tool encourage engineers to context switch?
> 
> _**In**tellectual complexity_
> 
> How much cognitive load is required to complete a task? What is the inherent complexity of the problem being solved? Do engineers need to deal with unnecessary complexity?
> 
> _**T**empo and velocity_
> 
> How quickly can engineers accomplish their tasks? How fast can they push their releases out? How many tasks do they complete in a given timeframe?
> 
> _**S**atisfaction_
> 
> How happy are engineers with their tools? How well does a tool meet engineers’ needs? How satisfied are they with their work and their end product? Are engineers feeling burned out?
> 
> Going back to the readability example, our research team worked with the readability team to identify several productivity goals of the readability process:
> 
> _Quality of the code_
> 
> Engineers write higher-quality code as a result of the readability process; they write more consistent code as a result of the readability process; and they contribute to a culture of code health as a result of the readability process.
> 
> _Attention from engineers_
> 
> We did not have any attention goal for readability. This is OK! Not all questions about engineering productivity involve trade-offs in all five areas.
> 
> _Intellectual complexity_
> 
> Engineers learn about the Google codebase and best coding practices as a result of the readability process, and they receive mentoring during the readability process.
> 
> _Tempo and velocity_
> 
> Engineers complete work tasks faster and more efficiently as a result of the readability process.
> 
> _Satisfaction_
> 
> Engineers see the benefit of the readability process and have positive feelings about participating in it.[^2]

### Signals

> A signal is the way in which we will know we’ve achieved our goal. Not all signals are measurable, but that’s acceptable at this stage. There is not a 1:1 relationship between signals and goals. Every goal should have at least one signal, but they might have more. Some goals might also share a signal.
> 
> | Goals | Signals |
> |:------|:--------|
> | Engineers write higher-quality code as a result of the readability process. | Engineers who have been granted readability judge their code to be of higher quality than engineers who have not been granted readability. The readability process has a positive impact on code quality.|
> | Engineers learn about the Google codebase and best coding practices as a result of the readability process.| Engineers report learning from the readability process. |
> | Engineers receive mentoring during the readability process.| Engineers report positive interactions with experienced Google engineers who serve as reviewers during the readability process. |
> | Engineers complete work tasks faster and more efficiently as a result of the readability process. | Engineers who have been granted readability judge themselves to be more productive than engineers who have not been granted readability. Changes written by engineers who have been granted readability are faster to review than changes written by engineers who have not been granted readability. |
> | Engineers see the benefit of the readability process and have positive feelings about participating in it. | Engineers view the readability process as being worthwhile. |
> 
> [^2]

### Metrics

> Metrics are where we finally determine how we will measure the signal. Metrics are not the signal themselves; they are the measurable proxy of the signal. Because they are a proxy, they might not be a perfect measurement. For this reason, some signals might have multiple metrics as we try to triangulate on the underlying signal.
> 
> For example, to measure whether engineers’ code is reviewed faster after readability, we might use a combination of both survey data and logs data. Neither of these metrics really provide the underlying truth. (Human perceptions are fallible, and logs metrics might not be measuring the entire picture of the time an engineer spends reviewing a piece of code or can be confounded by factors unknown at the time, like the size or difficulty of a code change.) However, if these metrics show different results, it signals that possibly one of them is incorrect and we need to explore further. If they are the same, we have more confidence that we have reached some kind
of truth.
> 
> Additionally, some signals might not have any associated metric because the signal might simply be unmeasurable at this time. Consider, for example, measuring code quality. Although academic literature has proposed many proxies for code quality, none of them have truly captured it. For readability, we had a decision of either using a poor proxy and possibly making a decision based on it, or simply acknowledging that this is a point that cannot currently be measured. Ultimately, we decided not to capture this as a quantitative measure, though we did ask engineers to self-rate their code quality.
> 
> Following the GSM framework is a great way to clarify the goals for why you are measuring your software process and how it will actually be measured. However, it’s still possible that the metrics selected are not telling the complete story because they are not capturing the desired signal. At Google, we use qualitative data to validate our metrics and ensure that they are capturing the intended signal.[^2]

### Using Data to Validate Metrics

> As an example, we once created a metric for measuring each engineer’s median build latency; the goal was to capture the “typical experience” of engineers’ build latencies. We then ran an _experience sampling study_. In this style of study, engineers are interrupted in context of doing a task of interest to answer a few questions. After an engineer started a build, we automatically sent them a small survey about their experiences and expectations of build latency. However, in a few cases, the engineers responded that they had not started a build! It turned out that automated tools were starting up builds, but the engineers were not blocked on these results and so it didn’t “count” toward their “typical experience.” We then adjusted the metric to exclude such builds.[^4]
> 
> Quantitative metrics are useful because they give you power and scale. You can measure the experience of engineers across the entire company over a large period of time and have confidence in the results. However, they don’t provide any context or narrative. Quantitative metrics don’t explain why an engineer chose to use an antiquated tool to accomplish their task, or why they took an unusual workflow, or why they circumvented a standard process. Only qualitative studies can provide this information, and only qualitative studies can then provide insight on the next steps to improve a process.
> 
> Consider now the signals presented in the table below. What metrics might you create to measure each of those? Some of these signals might be measurable by analyzing tool and code logs. Others are measurable only by directly asking engineers. Still others might not be perfectly measurable—how do we truly measure code quality, for example?
> 
> Ultimately, when evaluating the impact of readability on productivity, we ended up with a combination of metrics from three sources. First, we had a survey that was specifically about the readability process. This survey was given to people after they completed the process; this allowed us to get their immediate feedback about the process. This hopefully avoids recall bias,[^5] but it does introduce both recency bias[^6] and sampling bias.[^7] Second, we used a large-scale quarterly survey to track items that were not specifically about readability; instead, they were purely about metrics that we expected readability should affect. Finally, we used fine-grained logs metrics from our developer tools to determine how much time the logs claimed it took engineers to complete specific tasks.[^8] See below a complete list of metrics with their corresponding signals and goals.

| QUANTS | Goal | Signal | Metric |
|:-------|:-------|:-------|:-------|
| **Qu**ality of the code | Engineers write higher-quality code as a result of the readability process.| Engineers who have been granted readability judge their code to be of higher quality than engineers who have not been granted readability. | Quarterly Survey: Proportion of engineers who report being satisfied with the quality of their own code |
|  |  | The readability process has a positive impact on code quality. | Readability Survey: Proportion of engineers reporting that readability reviews have no impact or negative impact on code quality |
|  |  |  | Readability Survey: Proportion of engineers reporting that participating in the readability process has improved code quality for their team |

### Taking Action and Tracking Results

> Recall our original goal in this chapter: we want to take action and improve productivity. After performing research on a topic, the team at Google always prepares a list of recommendations for how we can continue to improve. We might suggest new features to a tool, improving latency of a tool, improving documentation, removing obsolete processes, or even changing the incentive structures for the engineers. Ideally, these recommendations are “tool driven”: it does no good to tell engineers to change their process or way of thinking if the tools do not support them in doing so. We instead always assume that engineers will make the appropriate trade-offs if they have the proper data available and the suitable tools at their disposal.
> 
> For readability, our study showed that it was overall worthwhile: engineers who had achieved readability were satisfied with the process and felt they learned from it. Our logs showed that they also had their code reviewed faster and submitted it faster, even accounting for no longer needing as many reviewers. Our study also showed places for improvement with the process: engineers identified pain points that would have made the process faster or more pleasant. The language teams took these recommendations and improved the tooling and process to make it faster and to be more transparent so that engineers would have a more pleasant experience.[^2]

## Appendix

### KRA Sheet: Performance Criteria for Non-Managers

| **Competency** | **1 - Unsatisfactory** | **2 - Sometimes Meets Requirements** | **3 - Meets Requirements Most of the Time** | **4 - Fully Meets and at Times Exceeds Requirements** | **5 - Consistently Exceeds Requirements** |
|-----------------------------------|------------------------|--------------------------------------|--------------------------------------------|--------------------------------------------------------|--------------------------------------------------|
| **1. Organizational Ability** | - Approaches tasks unsystematically. <br> - Wasteful. <br> - Output requires redoing to conform to standard format - this leads to increased costs. <br> - Fails to meet deadlines. | - Inconsistent in approach to tasks. <br> - Organizes tasks so deadlines are sometimes met but does not focus on cost implications. | - Systematic approach to tasks; frequently meets deadlines. <br> - Sensitive to cost management and seeks ways to reduce costs and improve efficiency. | - Logical, systematic, orderly approach to tasks. <br> - Consistently meets deadlines and applies quality control procedures to increase operational efficiency. <br> - Plays an active role in reducing costs. | - Develops priorities for tasks. <br> - Plans and schedules activities to meet deadlines. <br> - Consistently meets or surpasses deadlines. <br> - Develops innovative approaches to cost control. |
| **2. Quality of Output** | - Does not meet customer requirements. <br> - Work quality is below minimum acceptable standards - errors are excessive, evident. <br> - Requires continuous supervision and check back. <br> - Unreliable - does not perform duties despite continuous reminders. | - Frequently meets customer requirements. <br> - Output may occasionally require redoing/reworking. <br> - Requires supervision. <br> - Frequently slow. Does not exert enough effort. <br> - Duties performed in most cases but with reminders | - Always seeks to meet customer requirements. <br> - Output generated on time with minimal errors. <br> - Work quality always meets job demands. <br> - Satisfactory presentation <br> - Requires some supervision. <br> - Can be relied on to complete work with reasonable promptness | - Output always reflects meticulous in-depth study, analysis, or attention to detail. <br> - Proactive in determining needs and follows up to ensure delivery. <br> - Produces very neat work. <br> - Uses innovative approach. <br> - Low error rates. <br> - Minimal supervision required. <br> - Consistently reliable | - Proactive in determining customer needs and presents alternative courses of action. <br> - Consistently reliable with special assignments. <br> - Recognized for completeness and timeliness with low error rate. |
| **3. Communication** | - Has difficulty conveying facts or ideas with clarity. <br> - Does not listen. <br> - Writing has major errors. | - Conveys information but sometimes hard to follow. <br> - Does not always listen. <br> - Writing conveys the point but requires revision. | - Communicates facts clearly, audibly, and concisely. <br> - A good listener. <br> - Output is complete and accurate with few changes required. | - Communicates complex information well. <br> - Presentations are well-structured. <br> - Output is accurate, requiring few revisions. | - Articulate speaker; communicates complex information persuasively. <br> - Minimal revisions needed. <br> - Writing is concise and compelling. |
| **4. Interpersonal Skills** | - Disagreeable, cantankerous, and disruptive. <br> - Does not work well with others. | - Sometimes agreeable, but indifferent. <br> - Occasionally works with the team. | - Agreeable; avoids conflict. <br> - Functions as a team member when required. | - Actively participates as a team player; gains cooperation of team members. <br> - Handles difficult people with tact and grace. | - Maintains excellent relationships with colleagues. <br> - Handles conflicts when they arise. <br> - Recognized as a team leader. |
| **5. Analytical Ability** | - Unable to reason. <br> - Baffled by data. | - Analyzes routine issues well but requires instruction and assistance. | - Satisfactorily handles routine and non-standard information. | - Assimilates data but does not zero in on important issues. | - Quickly assimilates data and identifies key issues. <br> - Sought out by peers for analytical skills. |
| **6. Attendance and Punctuality** | - Frequently late and/or absent without permission. <br> - Seldom on the job. | - Occasionally late and/or absent without permission. <br> - Lapses unexpectedly away from the workplace. | - Rarely late or absent without permission. <br> - Observes regulations. | - Always punctual and on the job. <br> - Never absent without permission. | - Arrives before scheduled work time and leaves late. <br> - Never absent without permission. |
| **7. Deportment** | - Very untidy. <br> - Unprofessional behavior. <br> - Indiscreet. | - Somewhat careless about personal appearance. <br> - Business-like attitude but inconsistent. | - Satisfactory personal appearance. <br> - Conforms to established office standards. | - Good taste in dress; polite and professional. | - Unusually well-groomed, model of good behavior. <br> - Enhances the company's image. |
| **8. Customer Relations** | - Very curt, unpleasant, unconcerned in dealing with customers. <br> - Does not meet commitments. | - Helpful and pleasant but displays indifference at times. <br> - Inconsistent in meeting commitments. | - Always seeks to meet customer needs. <br> - Anticipates customer needs and offers assistance. | - Proactive in determining customer needs and going the extra mile. <br> - Polite and courteous. | - Proactive in determining customer needs and presents alternative courses of action. <br> - Handles difficult people with tact and grace. |
| **9. Job Attitude** | - Antagonistic. <br> - Poor influence on others. <br> - Lacks interest. | - Feels abused, inclined to disregard orders. <br> - Complains continually. | - Usually cooperative, diligent, and industrious. <br> - Seeks feedback. | - Willingly contributes extra energy and effort. <br> - Conscientious. | - Highly enthusiastic and persevering. <br> - Attempts to motivate others. |
| **10. Initiative** | - Little imagination or foresight. <br> - Heavily relies on direction from others. | - Rarely suggests improvements. <br> - Good follower but lacks initiative. | - Satisfactory ability to work within guidelines. | - Rather good at planning and improving efficiency. | - Always implementing ways to improve efficiency for the work unit. |
| **11. Internal Controls** | - No knowledge or regard for internal controls. <br> - Does not comply with policies. | - Some knowledge and occasionally complies with internal controls. | - Good knowledge and usually complies with policies and procedures. | - Excellent knowledge and consistently complies with internal controls. | - Considered an expert on internal controls. <br> - Always complies and encourages others to follow procedures. |
| **12. Departmental Objectives** | - Does not understand or appreciate departmental goals. <br> - Individualistic. | - Little appreciation or understanding of goals. <br> - Occasionally a team player. | - Appreciates and understands departmental objectives. <br> - Demonstrates team work. | - Major contributor to the department achieving its objectives. | - Motivates and trains others in the department. <br> - Considered a leader in the department. |

This table is a performance scoring guide that evaluates employees across different competencies, ranging from organizational ability and customer relations to job attitude and initiative. Each area is rated from unsatisfactory (1) to consistently exceeding requirements (5).

[^1]: Chapter 2. _How to Work Well on Teams_. Software Engineering at Google. Curated by Titus Winters, Tom Manshreck & Hyrum Wright
[^2]: Chapter 7. _Measuring Engineering Productivity_.Software Engineering at Google. Curated by Titus Winters, Tom Manshreck & Hyrum Wright
[^3]: [With Goals, FAST Beats SMART](https://sloanreview.mit.edu/article/with-goals-fast-beats-smart/) by Donald Sull and Charles Sull
[^4]: It has routinely been our experience at Google that when the quantitative and qualitative metrics disagree, it was because the quantitative metrics were not capturing the expected result.
[^5]: Recall bias is the bias from memory. People are more likely to recall events that are particularly interesting or frustrating.
[^6]: Recency bias is another form of bias from memory in which people are biased toward their most recent experience. In this case, as they just successfully completed the process, they might be feeling particularly good about it.
[^7]: Because we asked only those people who completed the process, we aren’t capturing the opinions of those who did not complete the process.
[^8]: There is a temptation to use such metrics to evaluate individual engineers, or perhaps even to identify high and low performers. Doing so would be counterproductive, though. If productivity metrics are used for performance reviews, engineers will be quick to game the metrics, and they will no longer be useful for measuring and improving productivity across the organization. The only way to make these measurements work is to let go of the idea of measuring individuals and embrace measuring the aggregate effect.